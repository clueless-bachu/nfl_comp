{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/kaggle/input/timm-0-6-9/pytorch-image-models-master')\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from helper import *\n",
    "tqdm.pandas()\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import albumentations as A\n",
    "import timm\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score, confusion_matrix\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "import torch\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from multiprocessing import Pool\n",
    "import random\n",
    "import cv2\n",
    "from math import ceil\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "tqdm.pandas()\n",
    "from glob import glob\n",
    "print('Done 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.65 s, sys: 632 ms, total: 4.28 s\n",
      "Wall time: 4.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "helmet_data= pd.read_csv('./train_baseline_helmets.csv')\n",
    "tracking_data = pd.read_csv('./train_player_tracking.csv')\n",
    "labels = pd.read_csv('./train_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.8 s, sys: 5.16 s, total: 33 s\n",
      "Wall time: 31.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "label_exp = expand_contact_id(labels)\n",
    "use_cols = [\n",
    "    'x_position', 'y_position', 'speed', 'distance',\n",
    "    'direction', 'orientation', 'acceleration', 'sa',\n",
    "    'team', 'position'\n",
    "]\n",
    "combined_df = create_features(label_exp, tracking_data, use_cols=use_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bboxes_details(frame, player1, player2, game_play, view):\n",
    "    '''\n",
    "    takes each row in comined_df and adds the required bbox information\n",
    "    (may need more processing to convert to correct datatype)\n",
    "    '''\n",
    "    bboxes = []\n",
    "    window = 24\n",
    "    query = f\"frame >= {frame-window}  and frame <= {frame+window} and game_play == '{game_play}'\"\n",
    "    if player2 == 'G':\n",
    "        query += f\"  and nfl_player_id in ({player1}, 'G')\"\n",
    "    else:\n",
    "        query += f\"  and nfl_player_id in ({player1}, {player2})\"\n",
    "    filt = helmet_data.query(query)\n",
    "\n",
    "    tmp = filt[filt[\"view\"] == view]\n",
    "    tmp = tmp.groupby('frame')[['left','width','top','height']].mean()\n",
    "    tmp['centre_x'] = tmp['left'] + tmp['width']/2\n",
    "    tmp['centre_y'] = tmp['top'] + tmp['height']/2\n",
    "    frame_list = tmp.index # these are the available frames\n",
    "    for fr in range(frame - window, frame + window + 1):\n",
    "        if fr in frame_list:\n",
    "            x, y = tmp.loc[fr][['centre_x','centre_y']]\n",
    "            bboxes.append([x, y])\n",
    "        else:\n",
    "            bboxes.append([np.nan, np.nan])\n",
    "    \n",
    "    bboxes = pd.DataFrame(np.array(bboxes)).interpolate(limit_direction='both').values\n",
    "    return np.array(bboxes)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, combined_df =train_test_split(\n",
    "        combined_df, test_size=6000, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [08:09<00:00, 12.25it/s]\n",
      "100%|██████████| 6000/6000 [08:14<00:00, 12.13it/s]\n"
     ]
    }
   ],
   "source": [
    "combined_df['bbox_endzone'] = combined_df.progress_apply(\n",
    "lambda x: add_bboxes_details(x['frame'],x['nfl_player_id_1'],x['nfl_player_id_2'],x['game_play'], 'Endzone'),\n",
    "axis=1\n",
    "    )\n",
    "combined_df['bbox_sideline'] = combined_df.progress_apply(\n",
    "lambda x: add_bboxes_details(x['frame'],x['nfl_player_id_1'],x['nfl_player_id_2'],x['game_play'], 'Sideline'),\n",
    "axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_image(path, cx, cy, view, aug):\n",
    "#     img_new = np.zeros((1, 256, 256), dtype=np.float32)\n",
    "#     # print(path)\n",
    "#     if os.path.isfile(path):\n",
    "#         if view == 'Endzone':\n",
    "#             img = cv2.imread(path, 0)[\n",
    "#                 cy-76:cy+180, cx-128:cx+128].copy()\n",
    "#             img_new[0, :img.shape[0], :img.shape[1]] = img\n",
    "#         else:\n",
    "#             img = cv2.imread(path, 0)[\n",
    "#                 cy-128:cy+128, cx-128:cx+128].copy()\n",
    "#             img_new[0, :img.shape[0], :img.shape[1]] = img\n",
    "\n",
    "#     return aug(image=img_new.transpose(1, 2, 0))['image'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'seed': 42,\n",
    "    'num_workers':4,\n",
    "    'batch_size': 64,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder()"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_aug = A.Compose([\n",
    "    A.Normalize(mean=[0.], std=[1.]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "categorical_data_for_fitting = [\n",
    "    ['home', 'CB', 'home', 'CB', ] ,\n",
    "    ['home', 'DE', 'home', 'DE', ],\n",
    "    ['home', 'FS', 'home', 'FS', ],\n",
    "    ['home', 'TE', 'home', 'TE', ] ,\n",
    "    ['home', 'ILB', 'home', 'ILB', ],\n",
    "    ['home', 'OLB', 'home', 'OLB', ],\n",
    "    ['home', 'T', 'home', 'T', ],\n",
    "    ['home', 'G', 'home', 'G', ] ,\n",
    "    ['home', 'C', 'home', 'C', ] ,\n",
    "    ['home', 'QB', 'home', 'QB', ],\n",
    "    ['home', 'WR', 'home', 'WR', ],\n",
    "    ['home', 'RB', 'home', 'RB', ],\n",
    "    ['home', 'NT', 'home', 'NT', ],\n",
    "    ['home', 'DT', 'home', 'DT', ],\n",
    "    ['home', 'MLB', 'home', 'MLB', ],\n",
    "    ['home', 'SS', 'home', 'SS', ] ,\n",
    "    ['home', 'OT', 'home', 'OT', ],\n",
    "    ['home', 'LB', 'home', 'LB', ],\n",
    "    ['home', 'OG', 'home', 'OG', ] ,\n",
    "    ['home', 'SAF', 'home', 'SAF', ],\n",
    "    ['home', 'DB', 'home', 'DB', ] ,\n",
    "    ['home', 'LS', 'home', 'LS', ] ,\n",
    "    ['home', 'K', 'home', 'K', ],\n",
    "    ['home', 'P', 'home', 'P', ],\n",
    "    ['home', 'FB', 'home', 'FB', ] ,\n",
    "    ['home', 'S', 'home', 'S', ],\n",
    "    ['home', 'DL', 'Ground', 'DL', ],\n",
    "    ['away', 'HB', 'away', 'HB', ],\n",
    "    ['away', 'HB', 'away', 'Ground', ],\n",
    "]\n",
    "    \n",
    "one_hot = OneHotEncoder()\n",
    "one_hot.fit(categorical_data_for_fitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset1(Dataset):\n",
    "    def __init__(self, df, aug, one_hot_transform, feature_cols=['rel_pos_x',\n",
    "                                                                 'rel_pos_y', 'rel_pos_mag', 'rel_pos_ori', 'rel_speed_x', 'rel_speed_y',\n",
    "                                                                 'rel_speed_mag', 'rel_speed_ori', 'rel_acceleration_x',\n",
    "                                                                 'rel_acceleration_y', 'rel_acceleration_mag', 'rel_acceleration_ori',\n",
    "                                                                 'G_flug', 'orientation_1', 'orientation_2']):\n",
    "\n",
    "        self.df = df\n",
    "        self.features = feature_cols\n",
    "        self.aug = aug\n",
    "        self.one_hot_transform = one_hot_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def normalize_features(self, features):\n",
    "        \"\"\"\n",
    "        normalizes the features of the players\n",
    "\n",
    "       'rel_pos_x',\n",
    "       'rel_pos_y', 'rel_pos_mag', 'rel_pos_ori', 'rel_speed_x', 'rel_speed_y',\n",
    "       'rel_speed_mag', 'rel_speed_ori', 'rel_acceleration_x',\n",
    "       'rel_acceleration_y', 'rel_acceleration_mag', 'rel_acceleration_ori',\n",
    "       'G_flug', 'orientation_1', 'orientation_2'\n",
    "        \"\"\"\n",
    "        features /= 100\n",
    "        features[3] /= 3.6\n",
    "        features[7] /= 3.6\n",
    "        features[11] /= 3.6\n",
    "        features[13] /= 3.6\n",
    "        features[14] /= 3.6\n",
    "        return features\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        window = 24\n",
    "        frames_to_skip = 4\n",
    "\n",
    "        row = self.df.iloc[idx]\n",
    "        mid_frame = row['frame']\n",
    "\n",
    "        label = float(row['contact'])\n",
    "        imgs = []\n",
    "        for view in ['Endzone', 'Sideline']:\n",
    "            video = row['game_play'] + f'_{view}.mp4'\n",
    "            frames = [mid_frame - window +\n",
    "                      i for i in range(0, 2*window+1, frames_to_skip)]\n",
    "\n",
    "            bbox_col = 'bbox_endzone' if view == 'Endzone' else 'bbox_sideline'\n",
    "            bboxes = row[bbox_col][::frames_to_skip].astype(np.int32)\n",
    "\n",
    "            if bboxes.sum() <= 0:\n",
    "                imgs += [np.zeros((256, 256), dtype=np.float32)]*len(frames)\n",
    "                continue\n",
    "\n",
    "            for i, frame in enumerate(frames):\n",
    "                img_new = np.zeros((256, 256), dtype=np.float32)\n",
    "                cx, cy = bboxes[i]\n",
    "                path = f'./work/train_frames/{video}_{frame:04d}.jpg'\n",
    "                if os.path.isfile(path):\n",
    "                    img_new = np.zeros((256, 256), dtype=np.float32)\n",
    "                    if view == 'Endzone':\n",
    "                        img = cv2.imread(path, 0)[\n",
    "                            cy-76:cy+180, cx-128:cx+128].copy()\n",
    "                        img_new[:img.shape[0], :img.shape[1]] = img\n",
    "                    else:\n",
    "                        img = cv2.imread(path, 0)[\n",
    "                            cy-128:cy+128, cx-128:cx+128].copy()\n",
    "                        img_new[:img.shape[0], :img.shape[1]] = img\n",
    "                imgs.append(img_new)\n",
    "\n",
    "        img = np.array(imgs).transpose(1, 2, 0)\n",
    "        img = self.aug(image=img)[\"image\"]\n",
    "\n",
    "        features = np.array(row[self.features], dtype=np.float32)\n",
    "        features[np.isnan(features)] = 0\n",
    "\n",
    "        \"\"\"\n",
    "        rel_pos_x                0\n",
    "        rel_pos_y                1\n",
    "        rel_pos_mag              2\n",
    "        rel_pos_ori              3\n",
    "        rel_speed_x              4\n",
    "        rel_speed_y              5\n",
    "        rel_speed_mag            6\n",
    "        rel_speed_ori            7\n",
    "        rel_acceleration_x       8\n",
    "        rel_acceleration_y       9\n",
    "        rel_acceleration_mag     10\n",
    "        rel_acceleration_ori     11 \n",
    "        \"\"\"\n",
    "        if row['G_flug']:\n",
    "            features[6] = row['speed_1']\n",
    "            features[7] = row['direction_1']\n",
    "            features[10] = row['acceleration_1']\n",
    "            features[11] = row['direction_1']\n",
    "\n",
    "            features[4] = row['speed_1']*np.sin(row['direction_1']*np.pi/180)\n",
    "            features[5] = row['speed_1']*np.cos(row['direction_1']*np.pi/180)\n",
    "            features[8] = row['acceleration_1'] * \\\n",
    "                np.sin(row['direction_1']*np.pi/180)\n",
    "            features[9] = row['acceleration_1'] * \\\n",
    "                np.cos(row['direction_1']*np.pi/180)\n",
    "        features = self.normalize_features(features)\n",
    "\n",
    "        team_pos = np.array(\n",
    "            row[['team_1', 'position_1', 'team_2', 'position_2']].fillna('Ground'))\n",
    "        team_pos = self.one_hot_transform.transform(\n",
    "            [team_pos]\n",
    "        ).toarray()[0]\n",
    "\n",
    "        return img, torch.from_numpy(np.hstack((features, team_pos)).astype(np.float32)), torch.as_tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 481/481 [00:11<00:00, 42.59it/s]\n"
     ]
    }
   ],
   "source": [
    "video2helmets = {}\n",
    "test_helmets_new = helmet_data.set_index('video')\n",
    "for video in tqdm(helmet_data.video.unique()):\n",
    "    video2helmets[video] = test_helmets_new.loc[video].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [01:12<00:00,  3.32it/s]\n"
     ]
    }
   ],
   "source": [
    "video2frames = {}\n",
    "test_video_metadata = pd.read_csv(\"./train_video_metadata.csv\")\n",
    "for game_play in tqdm(test_video_metadata.game_play.unique()):\n",
    "    for view in ['Endzone', 'Sideline']:\n",
    "        video = game_play + f'_{view}.mp4'\n",
    "        video2frames[video] = max(list(map(lambda x:int(x.split('_')[-1].split('.')[0]), \\\n",
    "                                           glob(f'./work/train_frames/{video}*'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset2(Dataset):\n",
    "    def __init__(self, df, aug=valid_aug, mode='test', transform=one_hot):\n",
    "        self.df = df\n",
    "        self.frame = df.frame.values\n",
    "        feature_cols = ['rel_pos_x','rel_pos_y', 'rel_pos_mag', 'rel_pos_ori', 'rel_speed_x', 'rel_speed_y',\n",
    "            'rel_speed_mag', 'rel_speed_ori', 'rel_acceleration_x', 'rel_acceleration_y', 'rel_acceleration_mag',\n",
    "            'rel_acceleration_ori', 'G_flug', 'orientation_1', 'orientation_2']\n",
    "        self.feature = df[feature_cols].fillna(0).values\n",
    "        self.players = df[['nfl_player_id_1','nfl_player_id_2']].values\n",
    "        self.game_play = df.game_play.values\n",
    "        self.aug = aug\n",
    "        self.mode = mode\n",
    "        self.one_hot_transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    # @lru_cache(1024)\n",
    "    # def read_img(self, path):\n",
    "    #     return cv2.imread(path, 0)\n",
    "    def normalize_features(self, features):\n",
    "        \"\"\"\n",
    "        normalizes the features of the players\n",
    "\n",
    "       'rel_pos_x',\n",
    "       'rel_pos_y', 'rel_pos_mag', 'rel_pos_ori', 'rel_speed_x', 'rel_speed_y',\n",
    "       'rel_speed_mag', 'rel_speed_ori', 'rel_acceleration_x',\n",
    "       'rel_acceleration_y', 'rel_acceleration_mag', 'rel_acceleration_ori',\n",
    "       'G_flug', 'orientation_1', 'orientation_2'\n",
    "        \"\"\"\n",
    "        features /= 100\n",
    "        features[3] /= 3.6\n",
    "        features[7] /= 3.6\n",
    "        features[11] /= 3.6\n",
    "        features[13] /= 3.6\n",
    "        features[14] /= 3.6\n",
    "        return features\n",
    "   \n",
    "    def __getitem__(self, idx):   \n",
    "        window = 24\n",
    "        frame = self.frame[idx]\n",
    "\n",
    "        players = []\n",
    "        for p in self.players[idx]:\n",
    "            if p == 'G':\n",
    "                players.append(p)\n",
    "            else:\n",
    "                players.append(int(p))\n",
    "        \n",
    "        imgs = []\n",
    "        for view in ['Endzone', 'Sideline']:\n",
    "            video = self.game_play[idx] + f'_{view}.mp4'\n",
    "\n",
    "            tmp = video2helmets[video]\n",
    "#             tmp = tmp.query('@frame-@window<=frame<=@frame+@window')\n",
    "            tmp[tmp['frame'].between(frame-window, frame+window)]\n",
    "            tmp = tmp[tmp.nfl_player_id.isin(players)]#.sort_values(['nfl_player_id', 'frame'])\n",
    "            tmp_frames = tmp.frame.values\n",
    "            tmp = tmp.groupby('frame')[['left','width','top','height']].mean()\n",
    "#0.002s\n",
    "\n",
    "            bboxes = []\n",
    "            for f in range(frame-window, frame+window+1, 1):\n",
    "                if f in tmp_frames:\n",
    "                    x, w, y, h = tmp.loc[f][['left','width','top','height']]\n",
    "                    bboxes.append([x, w, y, h])\n",
    "                else:\n",
    "                    bboxes.append([np.nan, np.nan, np.nan, np.nan])\n",
    "            bboxes = pd.DataFrame(bboxes).interpolate(limit_direction='both').values\n",
    "            bboxes = bboxes[::4]\n",
    "\n",
    "            if bboxes.sum() > 0:\n",
    "                flag = 1\n",
    "            else:\n",
    "                flag = 0\n",
    "#0.03s\n",
    "                    \n",
    "            for i, f in enumerate(range(frame-window, frame+window+1, 4)):\n",
    "                img_new = np.zeros((256, 256), dtype=np.float32)\n",
    "\n",
    "                if flag == 1 and f <= video2frames[video]:\n",
    "                    # print(f'/kaggle/work/frames/{video}_{f:04d}.jpg')\n",
    "                    img = cv2.imread(f'./work/train_frames/{video}_{f:04d}.jpg', 0)\n",
    "\n",
    "                    x, w, y, h = bboxes[i]\n",
    "                    if view == 'Endzone':\n",
    "                        img = img[int(y+h/2)-76:int(y+h/2)+180,int(x+w/2)-128:int(x+w/2)+128].copy()\n",
    "                    else:\n",
    "                        img = img[int(y+h/2)-128:int(y+h/2)+128,int(x+w/2)-128:int(x+w/2)+128].copy()\n",
    "                    img_new[:img.shape[0], :img.shape[1]] = img\n",
    "                    \n",
    "                imgs.append(img_new)\n",
    "\n",
    "        \n",
    "        features = np.float32(self.feature[idx])\n",
    "        # print(features.shape)\n",
    "        # print(\"this is i: \",i)\n",
    "        if self.df.iloc[idx]['G_flug']:\n",
    "                features[ 6] = self.df.iloc[idx]['speed_1']\n",
    "                features[7] = self.df.iloc[idx]['direction_1']\n",
    "                features[10] = self.df.iloc[idx]['acceleration_1']\n",
    "                features[11] = self.df.iloc[idx]['direction_1']\n",
    "\n",
    "                features[4] = self.df.iloc[idx]['speed_1'] * \\\n",
    "                    np.sin(self.df.iloc[idx]['direction_1']*np.pi/180)\n",
    "                features[5] = self.df.iloc[idx]['speed_1'] * \\\n",
    "                    np.cos(self.df.iloc[idx]['direction_1']*np.pi/180)\n",
    "                features[8] = self.df.iloc[idx]['acceleration_1'] * \\\n",
    "                    np.sin(self.df.iloc[idx]['direction_1']*np.pi/180)\n",
    "                features[9] = self.df.iloc[idx]['acceleration_1'] * \\\n",
    "                    np.cos(self.df.iloc[idx]['direction_1']*np.pi/180)\n",
    "        # print(\"unnormalized\",features)\n",
    "        features = self.normalize_features(features)\n",
    "        # print(features)\n",
    "\n",
    "        team_pos = np.array(\n",
    "            self.df.iloc[idx][['team_1', 'position_1', 'team_2', 'position_2']].fillna('Ground'))\n",
    "        team_pos = self.one_hot_transform.transform([team_pos]).toarray()[0]\n",
    "\n",
    "        img = np.array(imgs).transpose(1, 2, 0)    \n",
    "        img = self.aug(image=img)[\"image\"]\n",
    "        label = np.float32(self.df.contact.values[idx])\n",
    "\n",
    "        return img, torch.from_numpy(np.hstack((features, team_pos)).astype(np.float32)), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0310, 0.0151, 0.0345, 0.1777, 0.0297,\n",
      "        0.0145, 0.0331, 0.1777, 0.0100, 0.0636, 0.0000, 1.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0310, 0.0151, 0.0345, 0.1777, 0.0297,\n",
      "        0.0145, 0.0331, 0.1777, 0.0100, 0.0636, 0.0000, 1.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "test1 = MyDataset1(combined_df, aug=valid_aug, one_hot_transform=one_hot)\n",
    "test2 = MyDataset2(combined_df, aug=valid_aug, transform=one_hot)\n",
    "\n",
    "i = 13\n",
    "img1, feat1, label1 = test1[i]\n",
    "img2, feat2, label2 = test2[i]\n",
    "\n",
    "print(feat1)\n",
    "print(feat2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Validator1():\n",
    "    def __init__(self, test_df, aug, transform):\n",
    "        self.test_set = MyDataset1(\n",
    "            test_df, aug=aug, one_hot_transform=transform)\n",
    "        self.test_loader = DataLoader(\n",
    "            self.test_set, batch_size=CFG['batch_size'], num_workers=CFG['num_workers'], shuffle=False, pin_memory=False, persistent_workers=bool(CFG['num_workers']))\n",
    "\n",
    "    def validate(self, model):\n",
    "        y_hat = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.test_loader):\n",
    "                imgs, features, labels = batch\n",
    "\n",
    "                imgs = imgs.to(0, non_blocking=True)\n",
    "                features = features.to(0, non_blocking=True)\n",
    "\n",
    "                preds = model(imgs, features)\n",
    "                y_hat.append(preds.cpu().detach().numpy())\n",
    "\n",
    "            y_hat = np.hstack(y_hat)\n",
    "\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Validator2():\n",
    "    def __init__(self, df, aug, transform, verbose=True):\n",
    "        self.test_set = MyDataset2(\n",
    "            df, aug=aug, transform=transform)\n",
    "        self.test_loader = DataLoader(self.test_set, batch_size=CFG['batch_size'], shuffle=False,\\\n",
    "                                      num_workers=CFG['num_workers'], pin_memory=True)\n",
    "\n",
    "    def validate(self, model):\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            tk = tqdm(self.test_loader, total=len(self.test_loader))\n",
    "            for step, batch in enumerate(tk):\n",
    "                img, feature, label = [x.to(0) for x in batch]\n",
    "                # print(feature[-1])\n",
    "                output = model(img, feature).squeeze(-1)\n",
    "                # print(output)\n",
    "                y_pred.extend(output.cpu().numpy())\n",
    "\n",
    "        y_pred = np.array(y_pred)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "            'resnet50', pretrained=False, num_classes=250, in_chans=26)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(77, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        self.fc = nn.Linear(64+250, 1)\n",
    "\n",
    "    def forward(self, img, feature):\n",
    "        img = self.backbone(img)\n",
    "        feature = self.mlp(feature)\n",
    "        y = torch.sigmoid(self.fc(torch.cat([img, feature], dim=1)))\n",
    "        return y.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (backbone): ResNet(\n",
       "    (conv1): Conv2d(26, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act1): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "    (fc): Linear(in_features=2048, out_features=250, bias=True)\n",
       "  )\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=77, out_features=128, bias=True)\n",
       "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (5): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (6): ReLU()\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=314, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('./model_checkpoints/resnet50_fullforce_2023-02-12-01-02-29/best_model.pth')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "valer1 = Validator1(combined_df, aug=valid_aug, transform=one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "valer2 =  Validator2(combined_df, aug=valid_aug, transform=one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [01:33<00:00,  1.01it/s]\n"
     ]
    }
   ],
   "source": [
    "y_pred1 = valer1.validate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [01:54<00:00,  1.21s/it]\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = valer2.validate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([8.1473953e-11, 1.6620035e-04, 4.0028108e-06], dtype=float32),\n",
       " array([8.1473953e-11, 1.6620035e-04, 4.0028108e-06], dtype=float32))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred1[:3], y_pred2[:3],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fin1 = y_pred1 > 0.5\n",
    "y_fin2 = y_pred2 > 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_pred1 - y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.4413032e-06, 3.6112414e-08, 9.0477643e-09, ..., 3.3580517e-11,\n",
       "       5.6658714e-06, 5.9381913e-05], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(combined_df['contact'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8172961250261562"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matthews_corrcoef(labels, y_fin1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8172961250261562"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matthews_corrcoef(labels, y_fin2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9593333333333334, 0.9593333333333334)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(labels, y_fin1), accuracy_score(labels, y_fin2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5178,  236],\n",
       "       [   8,  578]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(labels, y_fin1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5178,  236],\n",
       "       [   8,  578]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(labels, y_fin2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfl_comp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e0971b6e4c6e31323d9ee278a729b5ea8905a211b067f5c5125d28ccbdd03cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
